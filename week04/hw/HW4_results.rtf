{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Consolas;\f2\fnil\fcharset0 Menlo-Bold;
}
{\colortbl;\red255\green255\blue255;\red27\green31\blue34;\red255\green255\blue255;\red27\green31\blue34;
\red244\green246\blue249;\red46\green174\blue187;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs32 \cf2 \cb3 1a. When adding a few red dots to previously green areas, the network does adjust, yet strategically placed red dots will cause some green dots to be outside the green area.\
1b. The algorithm has a more difficult time expanding the green area to meet added green dots.\
1c. 	Input - Input Layer. Holds raw values regarding the map.\
	FC(1) - Fully Connected Layer. Computes Class scores. \
	FC(2)  - Another Fully Connected Layer.\
	Softmax - Decision function, creating output based on class scores.\
1d.  Reducing the neuron count causes a reduction in accuracy. Accuracy is terrible with 1 neuron.\
1d. \cf4 \cb1 Increasing the neuron count causes an increase in accuracy, however, overfit is visually apparent at high neuron counts.\
1e. Tanh is fastest and most accurate. Relu is slower (than Tanh) and less accurate (than Tanh). Sigmoid is the slowest of all & poorest performer.\
2a.  Input, Conv (A filtering kernel is applied in this convolutional layer), Pool (Pooling is done to reduce parameters), Conv, Pool, Softmax\
	sx is a dimension of the input matrix\
	filters are kernel count applied\
	stride is the number of pixels to move for filter progression\
	pad is padding zeros to make picture \'91work\'92 for filter.\
	activation is activation function on a convolution layer.\
2b.  Increasing filter count can cause your computer too be overly stressed and lag! Reducing filter count causes a slowness in accuracy improvement.\
Increasing filter size helps learning. Reducing filter size causes a slowness in accuracy improvement.\
2c. Removing pooling layer increased accuracy at the expense of execution time.\
2d. Additional conv layer had marginal effect on accuracy.\
2e. Batch size increase had detrimental effect on accuracy.\
2f. Best accuracy achieved is .97. Learn rate = 5, Batch = 20, Remove 1 con & pool layer, remaining con sx = 15, filters = 20. \
\
\
Script for VSI\
\pard\pardeftab720\sl380
\ls1\ilvl0
\f1\fs28 \cf2 \cb5 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 ibmcloud sl vs create --datacenter=dal12 --hostname=hw04cpu --domain=sohn.com --os=UBUNTU_16_64 --flavor C1_8X8X100 --billing=hourly --san --disk=100 --disk=2000 --network 1000  --key=
\f2\b\fs22 \cf6 \cb1 \kerning1\expnd0\expndtw0 \CocoaLigature0 \outl0\strokewidth0 1704818
\f1\b0\fs28 \cf2 \cb5 \expnd0\expndtw0\kerning0
\CocoaLigature1 \outl0\strokewidth0 \strokec2 \
\pard\tx720\pardeftab720

\f0\fs32 \cf2 \cb1 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \
}